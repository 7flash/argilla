{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling a NER dataset for retraining with SpanMarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "from span_marker import SpanMarkerModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# Replace workspace with the name of your workspace\n",
    "rg.init(\n",
    "    api_url=\"https://ignacioct-argilla.hf.space\",\n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 8323\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1915\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1517\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super\")\n",
    "dataset = load_dataset(\"tomaarsen/conll2002\", \"es\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'document_id': 0, 'sentence_id': 0, 'tokens': ['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.'], 'pos_tags': [29, 21, 29, 22, 13, 59, 28, 21, 28, 22, 20], 'ner_tags': [5, 0, 5, 0, 0, 0, 0, 0, 3, 0, 0]}\n",
      "{'id': '1', 'document_id': 0, 'sentence_id': 1, 'tokens': ['-'], 'pos_tags': [16], 'ner_tags': [0]}\n",
      "{'id': '2', 'document_id': 0, 'sentence_id': 2, 'tokens': ['El', 'Abogado', 'General', 'del', 'Estado', ',', 'Daryl', 'Williams', ',', 'subrayó', 'hoy', 'la', 'necesidad', 'de', 'tomar', 'medidas', 'para', 'proteger', 'al', 'sistema', 'judicial', 'australiano', 'frente', 'a', 'una', 'página', 'de', 'internet', 'que', 'imposibilita', 'el', 'cumplimiento', 'de', 'los', 'principios', 'básicos', 'de', 'la', 'Ley', '.'], 'pos_tags': [4, 28, 1, 40, 28, 13, 47, 28, 13, 47, 38, 4, 28, 40, 49, 28, 40, 49, 40, 28, 1, 1, 38, 40, 7, 28, 40, 28, 35, 47, 4, 28, 40, 4, 28, 1, 40, 4, 28, 20], 'ner_tags': [0, 1, 2, 2, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0]}\n",
      "{'id': '3', 'document_id': 0, 'sentence_id': 3, 'tokens': ['La', 'petición', 'del', 'Abogado', 'General', 'tiene', 'lugar', 'después', 'de', 'que', 'un', 'juez', 'del', 'Tribunal', 'Supremo', 'del', 'estado', 'de', 'Victoria', '(', 'Australia', ')', 'se', 'viera', 'forzado', 'a', 'disolver', 'un', 'jurado', 'popular', 'y', 'suspender', 'el', 'proceso', 'ante', 'el', 'argumento', 'de', 'la', 'defensa', 'de', 'que', 'las', 'personas', 'que', 'lo', 'componían', 'podían', 'haber', 'obtenido', 'información', 'sobre', 'el', 'acusado', 'a', 'través', 'de', 'la', 'página', 'CrimeNet', '.'], 'pos_tags': [4, 28, 40, 28, 1, 47, 28, 38, 40, 3, 7, 28, 40, 28, 1, 40, 28, 40, 28, 21, 29, 22, 30, 51, 1, 40, 49, 7, 28, 1, 2, 49, 4, 28, 40, 4, 28, 40, 4, 28, 40, 3, 4, 28, 35, 34, 47, 47, 43, 50, 28, 40, 4, 50, 40, 28, 40, 4, 28, 1, 20], 'ner_tags': [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0]}\n",
      "{'id': '4', 'document_id': 0, 'sentence_id': 4, 'tokens': ['Esta', 'página', 'web', 'lleva', 'un', 'mes', 'de', 'existencia', ',', 'tiempo', 'en', 'el', 'que', 'ha', 'sido', 'visitada', 'en', 'más', 'de', 'un', 'millón', 'de', 'ocasiones', ',', 'y', 'facilita', 'información', 'sobre', 'miles', 'de', 'crímenes', 'y', 'criminales', 'ya', 'enjuiciados', 'o', 'aún', 'perseguidos', ',', 'datos', 'que', 'salen', 'de', 'artículos', 'de', 'periódicos', 'y', 'archivos', 'judiciales', '.'], 'pos_tags': [6, 28, 1, 47, 7, 28, 40, 28, 13, 28, 40, 4, 35, 41, 56, 50, 40, 38, 40, 7, 28, 40, 28, 13, 2, 47, 28, 40, 33, 40, 28, 2, 48, 38, 50, 2, 38, 48, 13, 28, 35, 47, 40, 28, 40, 28, 2, 28, 1, 20], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset[\"train\"].select(range(5)):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AO', 'AQ', 'CC', 'CS', 'DA', 'DE', 'DD', 'DI', 'DN', 'DP', 'DT', 'Faa', 'Fat', 'Fc', 'Fd', 'Fe', 'Fg', 'Fh', 'Fia', 'Fit', 'Fp', 'Fpa', 'Fpt', 'Fs', 'Ft', 'Fx', 'Fz', 'I', 'NC', 'NP', 'P0', 'PD', 'PI', 'PN', 'PP', 'PR', 'PT', 'PX', 'RG', 'RN', 'SP', 'VAI', 'VAM', 'VAN', 'VAP', 'VAS', 'VMG', 'VMI', 'VMM', 'VMN', 'VMP', 'VMS', 'VSG', 'VSI', 'VSM', 'VSN', 'VSP', 'VSS', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "fine_labels = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
    "print(fine_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignacio/opt/anaconda3/envs/argilla/lib/python3.9/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/ignacio/opt/anaconda3/envs/argilla/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'span': ['Melbourne'], 'label': 'location-GPE', 'score': 0.907048225402832, 'word_start_index': 0, 'word_end_index': 1}, {'span': ['Australia'], 'label': 'location-GPE', 'score': 0.9773393273353577, 'word_start_index': 2, 'word_end_index': 3}]\n",
      "[]\n",
      "[{'span': ['Daryl', 'Williams'], 'label': 'person-other', 'score': 0.7660976648330688, 'word_start_index': 6, 'word_end_index': 8}]\n",
      "[{'span': ['Victoria'], 'label': 'location-GPE', 'score': 0.6540813446044922, 'word_start_index': 18, 'word_end_index': 19}, {'span': ['Australia'], 'label': 'location-GPE', 'score': 0.97244793176651, 'word_start_index': 20, 'word_end_index': 21}, {'span': ['CrimeNet'], 'label': 'product-software', 'score': 0.3033939301967621, 'word_start_index': 59, 'word_end_index': 60}]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset[\"train\"].select(range(5)):\n",
    "    print(model.predict(sample['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cbe424ce18410ea983289ef825b6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> records logged to <a href=\"https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m20\u001b[0m records logged to \u001b]8;id=321581;https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\u001b\\\u001b[4;94mhttps://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='conll2002_es', processed=20, failed=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build records for the first 20 examples\n",
    "records = []\n",
    "\n",
    "for record in dataset[\"train\"].select(range(20)):\n",
    "\n",
    "    # Grouping up the raw text, the tokenized text and the predictions\n",
    "    predictions = model.predict(record['tokens'])\n",
    "    raw_text = \" \".join(record[\"tokens\"])\n",
    "    tokenized_text = record['tokens']   # we assume the text is split by spaces\n",
    "\n",
    "    # In the predictions we only have the starting and ending word indexes, but we need\n",
    "    # the character indexes to build TokenClassificationRecords. To obtain them, we have \n",
    "    # made a quick solution that searches for the star and end characters of each word \n",
    "    # and makes a list of tuples\n",
    "    word_indices = []\n",
    "    current_index = 0\n",
    "    for word in tokenized_text:\n",
    "        start = raw_text.find(word, current_index)\n",
    "        end = start + len(word)\n",
    "        current_index = end\n",
    "        word_indices.append((start, end))\n",
    "\n",
    "    # Now, we add these indexes to the predicions, to be able to append the predictions later.\n",
    "    for p in predictions:\n",
    "        p[\"start_char_index\"] = word_indices[p['word_start_index']][0]\n",
    "        p[\"end_char_index\"] = word_indices[p['word_end_index']-1][1]\n",
    "\n",
    "    # Building TokenClassificationRecord\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=raw_text,\n",
    "            tokens=tokenized_text,\n",
    "            prediction=[(p[\"label\"], p[\"start_char_index\"], p[\"end_char_index\"], p[\"score\"]) for p in predictions],\n",
    "            prediction_agent=\"tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Log the records to Argilla\n",
    "rg.log(records, name=\"conll2002_es\", metadata={\"split\": \"train\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
