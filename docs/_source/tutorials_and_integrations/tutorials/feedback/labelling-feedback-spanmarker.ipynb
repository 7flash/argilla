{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Labelling a NER dataset for retraining with SpanMarker\n",
    "\n",
    "In this example, we will show you how to integrate Argilla into a SpanMarker workflow to obtain a NER model, log its predictions to Argilla, manually marking correct and incorrect prediction and retrain your model. Human-in-the-loop!\n",
    "\n",
    "SpanMarker is a framework for training Named Entity Recognition models using familiar encoders such as BERT, RoBERTa... It's built on top of ü§ó Transformers, easy to easy, and making a pipeline integrating both SpanMarker and Argilla is a super powerful combination. Let's take a look on how to do it.\n",
    "\n",
    "**TODO: Write a summary of the workflow like in Mistral**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Argilla\n",
    "\n",
    "For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:\n",
    "\n",
    "**Deploy Argilla on Hugging Face Spaces:** If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:\n",
    "\n",
    "[![deploy on spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/deploy-to-spaces-lg.svg)](https://huggingface.co/new-space?template=argilla/argilla-template-space)\n",
    "\n",
    "For details about configuring your deployment, check the [official Hugging Face Hub guide](https://huggingface.co/docs/hub/spaces-sdks-docker-argilla).\n",
    "\n",
    "**Launch Argilla using Argilla's quickstart Docker image**: This is the recommended option if you want [Argilla running on your local machine](../../getting_started/quickstart.ipynb). Note that this option will only let you run the tutorial locally and not with an external notebook service.\n",
    "\n",
    "For more information on deployment options, please check the Deployment section of the documentation.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Tip\n",
    "\n",
    "This tutorial is a Jupyter Notebook. There are two options to run it:\n",
    "\n",
    "- Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference.\n",
    "- Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Let's start by installing the required dependencies to run both Argilla and the remainder of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install \"argilla\" \"spanmarker\" \"datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "rg.init(\n",
    "    api_url=\"https://ignacioct-argilla.hf.space\",\n",
    "    api_key=\"owner.apikey\",\n",
    "    workspace=\"admin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running a private Hugging Face Space, you will also need to set the \n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n",
    "[HF_TOKEN](https://huggingface.co/settings/tokens) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the HF_TOKEN environment variable\n",
    "# import os\n",
    "# os.environ['HF_TOKEN'] = \"your-hf-token\"\n",
    "\n",
    "# # Replace api_url with the url to your HF Spaces URL\n",
    "# # Replace api_key if you configured a custom API key\n",
    "# # Replace workspace with the name of your workspace\n",
    "# rg.init(\n",
    "#     api_url=\"https://hf-space.hf.space\", \n",
    "#     api_key=\"owner.apikey\",\n",
    "#     workspace=\"admin\",\n",
    "#     extra_headers={\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset chosen is a version of the [CoNLL-2002 dataset](https://huggingface.co/datasets/tomaarsen/conll2002) that highlights four types of entities: persons, locations, organizations and names of miscellaneous entities. This is a tagged example instance:\n",
    "\n",
    "```\n",
    "[PER Wolff] , currently a journalist in [LOC Argentina] , played with [PER Del Bosque] in the final years of the seventies in [ORG Real Madrid] .\n",
    "```\n",
    "\n",
    "This version of the dataset contains instances in both Spanish and Dutch, and has a size of 35k instances. Let's load it and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 8323\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1915\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document_id', 'sentence_id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1517\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"tomaarsen/conll2002\", \"es\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'document_id': 0,\n",
       " 'sentence_id': 0,\n",
       " 'tokens': ['La', 'Coru√±a', ',', '23', 'may', '(', 'EFECOM', ')', '.'],\n",
       " 'pos_tags': [4, 28, 13, 59, 28, 21, 29, 22, 20],\n",
       " 'ner_tags': [5, 6, 0, 0, 0, 0, 3, 0, 0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two types of entities in the dataset, PoS tags and NER tags. We will focus on the later. These NER tags follow a BIO scheme, meaning that each token can be a B tag (meaning that's the beggining of an entity), an I tag (an intermediate token of the entity), or an O tag (meaning that the token is a regular word). These scheme is encoded in the `features` field of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary that will come in handy later for translating list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a petrained model for SpanMarker.\n",
    "\n",
    "To make the initial predictions, we need a pretrained model to use alongside SpanMarker. We will do our first predictions in a zero-shot fashion, with no prior training. The model of choice, a [SpanMarker RoBERTa](https://huggingface.co/tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super) trained on the [FewNERD dataset](https://huggingface.co/datasets/DFKI-SLT/few-nerd), can be used directly on NER tasks, so it allows us to go straight to the prediction phase. Let's load it and do our first predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from span_marker import SpanMarkerModel\n",
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'Coru√±a', ',', '23', 'may', '(', 'EFECOM', ')', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'span': ['La', 'Coru√±a'],\n",
       "  'label': 'location-GPE',\n",
       "  'score': 0.929939329624176,\n",
       "  'word_start_index': 0,\n",
       "  'word_end_index': 2},\n",
       " {'span': ['EFECOM'],\n",
       "  'label': 'organization-media/newspaper',\n",
       "  'score': 0.7107717990875244,\n",
       "  'word_start_index': 6,\n",
       "  'word_end_index': 7}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['test'][0][\"tokens\"])\n",
    "model.predict(dataset['test'][0]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just made a zero-shot prediction, and quite accurate! Both entities are correctly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log predictions into Argilla\n",
    "\n",
    "Now that we show how to make predictions using the model, let's go a step forward and log some predictions into Argilla. We will work with a relatively small subset of the training split of this dataset to keep agility in performance. \n",
    "\n",
    "We will use `TokenClassificationDataset`, as our `FeedbackDataset` does not support NER tasks yet. But stay tunned for future releases, as we are planning on including this functionality for `FeedbackDatasets` as well! TokenClassificationDataset are made by TokenClassificationRecords, which, among several parameters, allow us to log the raw text, the tokens of that text, our predictions, and also some metadata to include extra information. We will use the metadata section to include the NER tags predicted by the model, so we can use a little hint in the annotation phase if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e515773bca465aa1f75cc1c0d4c488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> records logged to <a href=\"https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m20\u001b[0m records logged to \u001b]8;id=546977;https://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\u001b\\\u001b[4;94mhttps://ignacioct-argilla.hf.space/datasets/admin/conll2002_es\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='conll2002_es', processed=20, failed=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build records for the first 20 examples\n",
    "records = []\n",
    "\n",
    "for record in dataset[\"train\"].select(range(20)):\n",
    "\n",
    "    # Grouping up the raw text, the tokenized text and the predictions\n",
    "    predictions = model.predict(record['tokens'])\n",
    "    raw_text = \" \".join(record[\"tokens\"])\n",
    "    tokenized_text = record['tokens']   # we assume the text is split by spaces\n",
    "\n",
    "    # In the predictions we only have the starting and ending word indexes, but we need\n",
    "    # the character indexes to build TokenClassificationRecords. To obtain them, we have \n",
    "    # made a quick solution that searches for the star and end characters of each word \n",
    "    # and makes a list of tuples\n",
    "    word_indices = []\n",
    "    current_index = 0\n",
    "    for word in tokenized_text:\n",
    "        start = raw_text.find(word, current_index)\n",
    "        end = start + len(word)\n",
    "        current_index = end\n",
    "        word_indices.append((start, end))\n",
    "\n",
    "    # Now, we add these indexes to the predicions, to be able to append the predictions later.\n",
    "    for p in predictions:\n",
    "        p[\"start_char_index\"] = word_indices[p['word_start_index']][0]\n",
    "        p[\"end_char_index\"] = word_indices[p['word_end_index']-1][1]\n",
    "\n",
    "    # Building TokenClassificationRecord\n",
    "    records.append(\n",
    "        rg.TokenClassificationRecord(\n",
    "            text=raw_text,\n",
    "            tokens=tokenized_text,\n",
    "            prediction=[(p[\"label\"], p[\"start_char_index\"], p[\"end_char_index\"], p[\"score\"]) for p in predictions],\n",
    "            prediction_agent=\"tomaarsen/span-marker-xlm-roberta-base-fewnerd-fine-super\",\n",
    "            metadata={'dataset_gold_labels': [tag_dict[tag] for tag in record[\"ner_tags\"]]}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Log the records to Argilla\n",
    "rg.log(records, name=\"conll2002_es\", metadata={\"split\": \"train\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous snippet, we should have our first 20 predictions loaded in Argilla, ready for us to be annotated. Let's take a moment to go through the records and check if both the predictions made by our zero-shot model are correct, and if we can find more entities that have not been predicted by the model. In each record, if we press to the three-dot menu, we can choose to see more information on that specific record. By doing that, we can access the gold labels that we passed as metadata, in case of doubts.\n",
    "\n",
    "If you need help using the Argilla UI, you can go [here](https://docs.argilla.io/en/latest/reference/webapp/index.html) to find some documentation on the subject.\n",
    "\n",
    "![title](https://i.imgur.com/my74hnL.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
